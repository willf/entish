<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Word vectors: Machine learning oversimplified | Will&#39;s Whims</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ?">
    <meta name="generator" content="Hugo 0.94.2" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="Word vectors: Machine learning oversimplified" />
<meta property="og:description" content="Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/2015/05/12/word-vectors-machine-learning-oversimplified/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2015-05-12T19:36:43+00:00" />
<meta property="article:modified_time" content="2015-05-12T19:36:43+00:00" />

<meta itemprop="name" content="Word vectors: Machine learning oversimplified">
<meta itemprop="description" content="Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ?"><meta itemprop="datePublished" content="2015-05-12T19:36:43+00:00" />
<meta itemprop="dateModified" content="2015-05-12T19:36:43+00:00" />
<meta itemprop="wordCount" content="435">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word vectors: Machine learning oversimplified"/>
<meta name="twitter:description" content="Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ?"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Will&#39;s Whims
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>
    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Word vectors: Machine learning oversimplified</h1>
      
      <p class="tracked">
          By <strong>
          
              willwhim
          
          </strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2015-05-12T19:36:43Z">May 12, 2015</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><em>Word vectors</em> have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of <em>semantic</em> similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., <em>king</em> is to <em>queen</em> as <em>man</em> is to ? And hope that answer is <em>woman</em>.</p>
<p>Word vectors are often created using distributional data. That is, words that co-occur tend to be more similar to words that don’t co-occur. So, a typical second step is to create a co-occurrence matrix of probabilities among words. Second step? Yes, because the first step (as usual) is to break things up into “words” (tokenization). Different tokenization schemes will create different models, of course, and there’s no particular reason to limit the tokens to dictionary words.</p>
<p>So, one way to think about a word vector of <em>k</em> feature weights is that the dot product of the word vectors of two words should be their co-occurrence probability. So this becomes the objective function to create a learned model; details are in the <a href="http://nlp.stanford.edu/projects/glove/glove.pdf">GloVe paper</a> (pdf file).</p>
<p>These models can be computationally expensive to create. Fortunately, Stanford and Google have provided word vector models based on a variety of corpora (and various hyperparameters, such as the number of dimensions). Stanford provides <a href="http://nlp.stanford.edu/projects/glove/">word vectors</a> trained on Wikipedia plus the <a href="https://catalog.ldc.upenn.edu/LDC2011T07">Gigaword</a> corpus, <a href="https://commoncrawl.org/">Common Crawl</a>, and two billion Twitter tweets at various dimension sizes. Google’s <a href="https://code.google.com/p/word2vec/">word2vec</a> system provides word vectors trained on 100 billion words from Google News, and anoother set created from <a href="http://www.freebase.com/">Freebase</a> entities.</p>
<h2 id="example">Example</h2>
<p>I’m from Michigan, and my wife and I often noted the similarity of Michigan to Wisconsin. The Detroit of Wisconsin is Milwaukee (largest, industrial cities); the Ann Arbor of Wisconsin is Madison (liberal, best college towns), and so on. But the Lansing of Wisconsin is also Madison (capital cities).</p>
<p>Radim Řehůřek created a fun little <a href="http://radimrehurek.com/2014/02/word2vec-tutorial/#app">app</a> that uses the Google <code>word2vect</code> on the backend to do word analogies.</p>
<p>Here’s the <code>word2vec</code> results for the Michigan/Wisconsin comparisons:</p>
<ol>
<li><em>Michigan</em> is to <em>Detroit</em> as <em>Wisconsin</em> is to <strong>Milwaukee</strong>. (<em>yay!</em>)</li>
<li><em>Michigan</em> is to <em>Ann Arbor</em> as <em>Wisconsin</em> is to <strong>La Crosse</strong>. (<em>boo!</em>)</li>
<li><em>Michigan</em> is to <em>Lansing</em> as <em>Wisconsin</em> is to <strong>La Crosse</strong>. (<em>boo!</em>)</li>
<li><em>Michigan</em> is to <em>Grand Rapids</em> as <em>Wisconsin</em> is to <strong>La Crosse</strong>. (<em>boo!</em>)</li>
</ol>
<p>Ok, it seems to have a thing for La Cross. So, it’s not magic. But the fact that it’s producing somewhat similar cities (and not random words) is promising.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Will's Whims 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div></div>
  </div>
</footer>

  </body>
</html>
